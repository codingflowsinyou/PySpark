{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrames\n",
    "\n",
    "#### ¿Qué es?\n",
    "Es una colección de datos organizados en columnas, de manera análoga a la base de datos relacional.\n",
    "\n",
    "Su origen proviene de un objeto experimental introducido en Apache Spark 1.0 llamado *SchemaRDD* \n",
    "\n",
    "Para los usuarios familiarizados con R o Python, un Spark DataFrame es un concepto similar que permite que todos los usuarios trabajen fácilmente con estructuras de datos como tablas.\n",
    "\n",
    "Debido a la estructura que tienen, permite a los usuarios hacer queries en Spark SQL.\n",
    "\n",
    "Nota1: en antiguas versiones de Spark, ejecutar queries en Python resultaba muy costoso computacionalmente debido a la comunicación entre JVM y Py4j.\n",
    "\n",
    "Nota2: En antiguas versiones de Spark, para ejecutar queries se utilizaba *SQL Context*, ahora se ha unificado *HiveContext*, *SQLContext*, *StreamingContext* y *SparkContext* en *SparkSession*. Más información How to use SparkSession in Apache Spark 2.0(http://bit.ly/2br0Fr1).\n",
    "\n",
    "\n",
    "#### Python to RDD communications\n",
    "En el siguiente diagrama se muestra que en el driver de PySpark, Spark Context utiliza Py4j para lanzar la JVM utilizando JavaSparkContext. Cualquier transformación de RDD son inicialmente PythonRDD objetos en Java.\n",
    "\n",
    "Una vez esas tasks abandonan los workers, objetos PythonRDD lanzan subprocesos utilizando pipes para enviar tanto código como datos y así ser procesados dentro de Python.\n",
    "(diagrama hoja papel)\n",
    "\n",
    "<img src=\"python_spark.png\">\n",
    "\n",
    "\n",
    "Mientras que esta aproximación permite a PySpark distribuir el procesamiento de los datos en múltiples Python subprocesses en multiple workers, como se puede observar, hay muchísima comunicación intercambiandose entre Python y JVM.\n",
    "\n",
    "Spark performance beyond the JVM: http://bit.ly/2bx89bn.\n",
    "\n",
    "#### Spark SQL\n",
    "\n",
    "Spark SQL es un módulo de Spark para el procesamiento de datos estructurados. A diferencia de la API básica de Spark RDD, las interfaces proporcionadas por Spark SQL brindan a Spark más información sobre la estructura tanto de los datos como del cálculo que se está realizando. Internamente, Spark SQL usa esta información adicional para realizar optimizaciones adicionales. Hay varias formas de interactuar con Spark SQL, incluidas SQL y la API de conjunto de datos. Al calcular un resultado, se utiliza el mismo motor de ejecución, independientemente de qué API / lenguaje esté utilizando para expresar el cálculo. Esta unificación significa que los desarrolladores pueden alternar fácilmente entre diferentes API en función de cuál proporciona la forma más natural de expresar una transformación determinada.\n",
    "\n",
    "Un uso de Spark SQL es ejecutar consultas SQL. Spark SQL también se puede utilizar para leer datos de una instalación de Hive existente.  Cuando se ejecuta SQL desde otro lenguaje de programación, los resultados se devolverán como un Dataset / DataFrame.\n",
    "\n",
    "A Spark SQL se le llama **Catalyst Optimizer** \n",
    "+ info en Cost-based Optimizer Framework at https://issues.apache.org/jira/browse/SPARK-16026\n",
    "the Design Specification of Spark Cost-Based Optimization at http://bit.ly/2li1t4T.\n",
    "\n",
    "Falta desarrollo.\n",
    "**Proyect Tungsten** hay una gran cantidad de mejoras de la performance generando a byte code (code generation or codegen) en vez de interpretar cada row de los datos. \n",
    "El optimizador está basado en constructores de programación funcional y fue diseñado con dos propósitos en mente: facilitar la adición de nuevas técnicas de optimización y características de Spark SQL, y permitir a desarrolladores externos extender y mejorar el optimizador.\n",
    "Structuring Spark: SQL DataFrames, Datasets, and Streaming at http://bit.ly/2cJ508x.]]**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating DataFrames\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### arreglos\n",
    "import os\n",
    "os.environ['PYSPARK_PYTHON'] = '/opt/anaconda3/bin/python'\n",
    "os.environ['PYSPARK_DRIVER_PYTHON']='/opt/anaconda3/bin/python'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inicializar\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark SQL basic example\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generar JSON data\n",
    "Generaremos inicialmente un `stringJSONRDD`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "stringJSONRDD = sc.parallelize((\"\"\"\n",
    "  { \"id\": \"123\",\n",
    "\"name\": \"Katie\",\n",
    "\"age\": 19,\n",
    "\"eyeColor\": \"brown\"\n",
    "  }\"\"\",\n",
    "\"\"\"{\n",
    "\"id\": \"234\",\n",
    "\"name\": \"Michael\",\n",
    "\"age\": 22,\n",
    "\"eyeColor\": \"green\"\n",
    "  }\"\"\", \n",
    "\"\"\"{\n",
    "\"id\": \"345\",\n",
    "\"name\": \"Simone\",\n",
    "\"age\": 23,\n",
    "\"eyeColor\": \"blue\"\n",
    "  }\"\"\")\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "swimmersJSON = spark.read.json(stringJSONRDD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating a temporary table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# desaparecerá si la sesión se cierra\n",
    "swimmersJSON.createOrReplaceTempView(\"swimmersJSON\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para entender mejor el funcionamiento visualmente:\n",
    "Understanding Your Apache Spark Application Through Visualization at http://bit.ly/2cSemkv.\n",
    "\n",
    "Es importante entender que `paralellize`, `map`y `mapPartitions` son todos RDD transformaciones. En este caso, es importante entender que `spark.read.json` no son solo RDDs transformations, también son acciones para convertir el RDD en dataframes. Esto es importante, ya que si estás ejecutando una DataFrame operation, para debug tus operaciones necesitarás entender que estarás haciendo RDD operaciones dentro de Spark UI.\n",
    "\n",
    "#### Simple DataFrame queries\n",
    "Una vez tenemos creado el swimmersJSON, podremos ejecutar el DataFrame API "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+---+-------+\n",
      "|age|eyeColor| id|   name|\n",
      "+---+--------+---+-------+\n",
      "| 19|   brown|123|  Katie|\n",
      "| 22|   green|234|Michael|\n",
      "| 23|    blue|345| Simone|\n",
      "+---+--------+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Running the .show() method will default to present the first 10 rows\n",
    "# DataFrame API\n",
    "swimmersJSON.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SQL query\n",
    "Se puede escribir en SQL statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(age=19, eyeColor='brown', id='123', name='Katie'),\n",
       " Row(age=22, eyeColor='green', id='234', name='Michael'),\n",
       " Row(age=23, eyeColor='blue', id='345', name='Simone')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM swimmersJSON\").collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interoperating with RDDs\n",
    "##### 1. Inferring the schema using reflection\n",
    "En el proceso de construir un DataFrame y hacer queries, olvidemonos de la idea de que el esquema del DataFrame estaba automáticamente definido. Inicialmente, las rows objects están construidas pasando una lista de key/value pairs como ** kwars . Entonces, SparkSQL convierte este RDD de row en un DataFrame donde las keys son las columnas y los datatypes están inferidos by sampling the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: long (nullable = true)\n",
      " |-- eyeColor: string (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print the schema\n",
    "swimmersJSON.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. Programmatically specifying the schema\n",
    "Vamos a programar el esquema con ayuda de los SparkSQL types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import types\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comma delimited\n",
    "stringCSVRDD = sc.parallelize([\n",
    "(123, 'Katie', 19, 'brown'), \n",
    "(234, 'Michael', 22, 'green'), \n",
    "(345, 'Simone', 23, 'blue')\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero, we will encode the schema as a string, per the [schema] variable below. Then we will define the schema using `StructType` and `StructField`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify schema\n",
    "schema = StructType([\n",
    "StructField(\"id\", LongType(), True),    \n",
    "StructField(\"name\", StringType(), True),\n",
    "StructField(\"age\", LongType(), True),\n",
    "StructField(\"eyeColor\",\n",
    "StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notar que `StructField` class se divide en términos de:\n",
    "- name: el nombre del campo\n",
    "- dataType: the dataType of this field.\n",
    "- nullable: Indica si el campo puede ser nulo o no."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the schema to the RDD and Create DataFrame\n",
    "swimmers = spark.createDataFrame(stringCSVRDD, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a temporary view using the DataFrame\n",
    "swimmers.createOrReplaceTempView(\"swimmers\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      " |-- eyeColor: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "swimmers.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Querying with the DataFrame API\n",
    "##### Number of rows\n",
    "Utilizamos `count()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "swimmers.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Running filter statements\n",
    "Utilizamos `filter` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "| id|age|\n",
      "+---+---+\n",
      "|234| 22|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get the id, age where age = 22\n",
    "swimmers.select(\"id\", \"age\").filter(\"age = 22\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "| id|age|\n",
      "+---+---+\n",
      "|234| 22|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Another way to write the above query is below\n",
    "swimmers.select(swimmers.id, swimmers.age).filter(swimmers.age == 22).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+\n",
      "|  name|eyeColor|\n",
      "+------+--------+\n",
      "| Katie|   brown|\n",
      "|Simone|    blue|\n",
      "+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get the name, eyeColor where eyeColor like 'b%'\n",
    "swimmers.select(\"name\", \"eyeColor\").filter(\"eyeColor like 'b%'\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Querying with SQL\n",
    "##### Number of rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|       3|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select count(1) from swimmers\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Running filter statements using the where Clauses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "| id|age|\n",
      "+---+---+\n",
      "|234| 22|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get the id, age where age = 22 in SQL\n",
    "spark.sql(\"select id, age from swimmers where age = 22\").show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+\n",
      "|  name|eyeColor|\n",
      "+------+--------+\n",
      "| Katie|   brown|\n",
      "|Simone|    blue|\n",
      "+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select name, eyeColor from swimmers where eyeColor like 'b%'\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nota importante: si alguna vez queremos guardar una tabla y que preserve las condiciones iniciales lo mejor es guardarlo en `Parquet files`\n",
    "For more information, please refer to the latest Spark SQL Programming Guide > Parquet Files at: http://spark.apache.org/docs/latest/sql-programming-guide.html#parquet-files.\n",
    "\n",
    "Automatic Partition Discovery and Schema Migration for Parquet at https://databricks.com/blog/2015/03/24/spark-sql-graduates-from-alpha-in-spark-1-3.html and How Apache Spark performs a fast count using the parquet metadata at https://github.com/dennyglee/databricks/blob/master/misc/parquet-count-metadata-explanation.md.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DataFrame scenario - on time flight performance\n",
    "Analizamos Airline On-Time Performance and Causes of Flight Delays: On-Time Data\n",
    "bit.ly/2ccJPPM), and join this with the airports dataset, obtained from the Open Flights Airport, airline, and route data (http://bit.ly/2ccK5hw), to better understand the variables associated with flight delays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[date: string, delay: string, distance: string, origin: string, destination: string]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set File Paths\n",
    "flightPerfFilePath = \"./learningPySpark/Data/departuredelays.csv\"\n",
    "airportsFilePath = \"./learningPySpark/Data/airport-codes-na.txt\"\n",
    "\n",
    "# Obtain Airports dataset\n",
    "airports = spark.read.csv(airportsFilePath, header='true', inferSchema='true', sep='\\t')\n",
    "airports.createOrReplaceTempView(\"airports\")\n",
    "\n",
    "# Obtain Departure Delays dataset\n",
    "flightPerf = spark.read.csv(flightPerfFilePath, header='true')\n",
    "flightPerf.createOrReplaceTempView(\"FlightPerformance\")\n",
    "\n",
    "# Cache the Departure Delays dataset ? esto que quiere decir ?\n",
    "flightPerf.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Joining flight performance and airports\n",
    "Una de las tareas más comunes es hacer joins entre dataframes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+--------+\n",
      "|   City|origin|  Delays|\n",
      "+-------+------+--------+\n",
      "|Seattle|   SEA|159086.0|\n",
      "|Spokane|   GEG| 12404.0|\n",
      "|  Pasco|   PSC|   949.0|\n",
      "+-------+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Query Sum of Flight Delays by City and Origin Code (for Washington State)\n",
    "spark.sql(\"select a.City, f.origin, sum(f.delay) as Delays from FlightPerformance f join airports a on a.IATA = f.origin where a.State = 'WA' group by a.City, f.origin order by sum(f.delay) desc\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### + Ejemplos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+\n",
      "|   name|(age + 1)|\n",
      "+-------+---------+\n",
      "|  Katie|       20|\n",
      "|Michael|       23|\n",
      "| Simone|       24|\n",
      "+-------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select everybody, but increment the age by 1\n",
    "swimmers.select(swimmers['name'], swimmers['age'] + 1).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+---+--------+\n",
      "| id|   name|age|eyeColor|\n",
      "+---+-------+---+--------+\n",
      "|234|Michael| 22|   green|\n",
      "|345| Simone| 23|    blue|\n",
      "+---+-------+---+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select people older than 21\n",
    "swimmers.filter(swimmers['age'] > 21).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "|age|count|\n",
      "+---+-----+\n",
      "| 19|    1|\n",
      "| 22|    1|\n",
      "| 23|    1|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Count people by age\n",
    "swimmers.groupBy(\"age\").count().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of rows: 7\n",
      "Count of distinct rows: 6\n"
     ]
    }
   ],
   "source": [
    "# eliminar duplicados\n",
    "df = spark.createDataFrame([(1, 144.5, 5.9, 33, 'M'),\n",
    "        (2, 167.2, 5.4, 45, 'M'),\n",
    "        (3, 124.1, 5.2, 23, 'F'),\n",
    "        (4, 144.5, 5.9, 33, 'M'),\n",
    "        (5, 133.2, 5.7, 54, 'F'),\n",
    "        (3, 124.1, 5.2, 23, 'F'),\n",
    "        (5, 129.2, 5.3, 42, 'M'),], ['id', 'weight', 'height', 'age', 'gender'])\n",
    "\n",
    "print('Count of rows: {0}'.format(df.count()))\n",
    "print('Count of distinct rows: {0}'.format(df.distinct().count()))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of rows: 6\n",
      "Count of distinct rows: 6\n"
     ]
    }
   ],
   "source": [
    "df = df.dropDuplicates()\n",
    "print('Count of rows: {0}'.format(df.count()))\n",
    "print('Count of distinct rows: {0}'.format(df.distinct().count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of ids: 6\n",
      "Count of distinct ids: 5\n"
     ]
    }
   ],
   "source": [
    "# por Ids\n",
    "print('Count of ids: {0}'.format(df.count()))\n",
    "print('Count of distinct ids: {0}'.format(\n",
    "    df.select([\n",
    "        c for c in df.columns if c != 'id'\n",
    "    ]).distinct().count())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+---+------+\n",
      "| id|weight|height|age|gender|\n",
      "+---+------+------+---+------+\n",
      "|  5| 133.2|   5.7| 54|     F|\n",
      "|  1| 144.5|   5.9| 33|     M|\n",
      "|  2| 167.2|   5.4| 45|     M|\n",
      "|  3| 124.1|   5.2| 23|     F|\n",
      "|  5| 129.2|   5.3| 42|     M|\n",
      "+---+------+------+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.dropDuplicates(subset=[c for c in df.columns if c != 'id'])\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+\n",
      "|count|distinct|\n",
      "+-----+--------+\n",
      "|    5|       4|\n",
      "+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# agregar funciones\n",
    "import pyspark.sql.functions as fn\n",
    "    \n",
    "df.agg(fn.count('id').alias('count'),\n",
    "    fn.countDistinct('id').alias('distinct')\n",
    ").show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+---+------+-------------+\n",
      "| id|weight|height|age|gender|       new_id|\n",
      "+---+------+------+---+------+-------------+\n",
      "|  5| 133.2|   5.7| 54|     F|  25769803776|\n",
      "|  1| 144.5|   5.9| 33|     M| 171798691840|\n",
      "|  2| 167.2|   5.4| 45|     M| 592705486848|\n",
      "|  3| 124.1|   5.2| 23|     F|1236950581248|\n",
      "|  5| 129.2|   5.3| 42|     M|1365799600128|\n",
      "+---+------+------+---+------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn('new_id', fn.monotonically_increasing_id()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
